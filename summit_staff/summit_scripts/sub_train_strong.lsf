#!/bin/bash
# Begin LSF directives
#BSUB -P stf011
#BSUB -J cosmoflow
#BSUB -o logs/cosmoflow.o%J
#BSUB -W 0:30
#BSUB -nnodes 10
#BSUB -alloc_flags "nvme smt4"
#BSUB -N
# End LSF directives and begin shell commands

nnodes=$(cat ${LSB_DJOB_HOSTFILE} | sort | uniq | grep -v login | grep -v batch | wc -l)

echo "Setup env"
export PATH=/ccs/home/atsaris/.conda/envs/myclone/bin/:$PATH
export LD_LIBRARY_PATH=/ccs/home/atsaris/.conda/envs/myclone/bin/:$LD_LIBRARY_PATH

echo "Copy files"
jsrun -n${nnodes} -a1 -c42 -r1 cp /gpfs/alpine/stf011/proj-shared/atsaris/logs/cosmoUniverse_2019_02_4parE/dim128_cube_nT4/cosmoUniverse_2019_02_4parE-dim128_cube_nT4-rec11* /mnt/bb/$USER

echo "Train multi node scalability"
jsrun -n2 -a6 -c42 -g6 -r1 --bind=proportional-packed:7 --launch_distribution=packed stdbuf -o0 python train.py -d --rank-gpu summit_scripts/scaling.yaml --data-config "{n_train_files: 64}" --output-dir /gpfs/alpine/stf011/proj-shared/atsaris/logs/cosmoflow_2020/log_12
jsrun -n4 -a6 -c42 -g6 -r1 --bind=proportional-packed:7 --launch_distribution=packed stdbuf -o0 python train.py -d --rank-gpu summit_scripts/scaling.yaml --data-config "{n_train_files: 64}" --output-dir /gpfs/alpine/stf011/proj-shared/atsaris/logs/cosmoflow_2020/log_24
jsrun -n8 -a6 -c42 -g6 -r1 --bind=proportional-packed:7 --launch_distribution=packed stdbuf -o0 python train.py -d --rank-gpu summit_scripts/scaling.yaml --data-config "{n_train_files: 64}" --output-dir /gpfs/alpine/stf011/proj-shared/atsaris/logs/cosmoflow_2020/log_32
jsrun -n10 -a6 -c42 -g6 -r1 --bind=proportional-packed:7 --launch_distribution=packed stdbuf -o0 python train.py -d --rank-gpu summit_scripts/scaling.yaml --data-config "{n_train_files: 64}" --output-dir /gpfs/alpine/stf011/proj-shared/atsaris/logs/cosmoflow_2020/log_60

